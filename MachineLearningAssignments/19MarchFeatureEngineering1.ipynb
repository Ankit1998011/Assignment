{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee16c1e1-9103-4337-8bf3-44b7fd1aac2c",
   "metadata": {},
   "source": [
    "### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "### algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d1591-9eb0-4493-bb0a-710c6698f5dd",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "Missing values refer to the absence of values or data points in a dataset. In other words, when a dataset is incomplete, and some entries do not contain any data, these entries are referred to as missing values. \n",
    "\n",
    "It is essential to handle missing values because they can affect the accuracy of statistical analysis and machine learning models. If we ignore missing values, it can lead to biased or incorrect results, as missing values can change the distribution of the data, and impact the validity of our conclusions. Therefore, it is crucial to handle missing values in a meaningful way to obtain accurate and reliable insights from the dataset.\n",
    "\n",
    "Here are some commonly used algorithms that are not affected by missing values:\n",
    "\n",
    "1.Tree-based models: Decision trees and Random Forests are examples of tree-based models that can handle missing values without requiring any pre-processing. They can work with both categorical and numerical data types.\n",
    "\n",
    "2.K-nearest neighbor (KNN): KNN is a non-parametric algorithm that can handle missing values by ignoring the missing attributes while computing the distance between instances. \n",
    "\n",
    "3.Support Vector Machines (SVMs): SVMs can handle missing values by ignoring them or by replacing them with an appropriate value. \n",
    "\n",
    "4.Gaussian Mixture Models (GMMs): GMMs can handle missing values by computing the posterior probabilities of missing values given the observed data and model parameters.\n",
    "\n",
    "5.Naive Bayes: Naive Bayes can handle missing values by ignoring them or by assigning a probability to the missing value based on the probabilities of observed values. \n",
    "\n",
    "Overall, it is essential to handle missing values appropriately based on the nature of the data, the purpose of the analysis, and the algorithm used to obtain accurate and reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b51f7a-3a2f-4660-a11a-89e37db84d11",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7fd9a2-3460-4b5d-93ee-04357d5537ec",
   "metadata": {},
   "source": [
    "### Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16111366-930c-4a63-8dbd-6d72c8fa1aa0",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "There are several techniques that can be used to handle missing data. Here are some commonly used techniques with Python code examples:\n",
    "\n",
    "1.Deletion: This technique involves removing the rows or columns with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d419c0-8a8e-4c35-8d6b-49114d807648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "1  2.0  6.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "#Example:-\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a sample dataset\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 6, 7, 8]})\n",
    "\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04894267-196f-4f8b-b654-875f64bdb26e",
   "metadata": {},
   "source": [
    "2.Imputation: This technique involves filling the missing data with estimated values. It can be done in several ways:\n",
    "\n",
    "->Mean imputation: Replace missing values with the mean of the available values.\n",
    "\n",
    "->Median imputation: Replace missing values with the median of the available values.\n",
    "\n",
    "->Mode imputation: Replace missing values with the mode of the available values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d55b11-07de-4405-afc0-96b9a73ad630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Imputation:-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44477e4-30e6-4c58-86be-11bf014ade41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "199cb51d-4574-449b-b3a3-11f985f6a39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"age\"].isnull().sum()  # number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f3d020-8930-4dfa-bfb2-b69d9679ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df[\"age\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3903616c-b23f-42c4-98ab-8413c3b9019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new age column with mean imputation:-\n",
    "\n",
    "df[\"age_mean\"] = df[\"age\"].fillna(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5471ed5-f795-47a2-ae3b-5cca0c888218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>27.0</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>19.0</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>26.0</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>32.0</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age   age_mean\n",
       "0    22.0  22.000000\n",
       "1    38.0  38.000000\n",
       "2    26.0  26.000000\n",
       "3    35.0  35.000000\n",
       "4    35.0  35.000000\n",
       "..    ...        ...\n",
       "886  27.0  27.000000\n",
       "887  19.0  19.000000\n",
       "888   NaN  29.699118\n",
       "889  26.0  26.000000\n",
       "890  32.0  32.000000\n",
       "\n",
       "[891 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"age\",\"age_mean\"]] # missing value replaced with mean age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03151d0b-28e9-4dc6-a94e-29972363325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median imputation:-\n",
    "\n",
    "median = df[\"age\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae8c96d4-2547-487f-9bf6-b883d0a8d262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46290155-bea6-4df4-98fe-93710fc4bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new age column with mean imputation:-\n",
    "\n",
    "df[\"age_median\"] = df[\"age\"].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44164349-d82d-4fa8-864f-8216a929cf79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  age_median\n",
       "0    22.0        22.0\n",
       "1    38.0        38.0\n",
       "2    26.0        26.0\n",
       "3    35.0        35.0\n",
       "4    35.0        35.0\n",
       "..    ...         ...\n",
       "886  27.0        27.0\n",
       "887  19.0        19.0\n",
       "888   NaN        28.0\n",
       "889  26.0        26.0\n",
       "890  32.0        32.0\n",
       "\n",
       "[891 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"age\", \"age_median\"]]    #missing value replaced with mean age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7ffb7f4-b51d-4566-8f65-493c1b8c1a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mode imputation:-\n",
    "\n",
    "df[\"embark_town\"].isnull().sum() #number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb2c5fd3-5f36-4d75-b28f-4e29e184cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = df[df[\"age\"].notna()][\"embark_town\"].mode()[0]  #calculating mode from data where age is not missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b93d6508-6dbf-4d3b-aa01-09c99e9048aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"new_embark_town\"] = df[\"embark_town\"].fillna(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37c2585a-6224-4df6-b886-9ee053e787ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embark_town</th>\n",
       "      <th>new_embark_town</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    embark_town new_embark_town\n",
       "61          NaN     Southampton\n",
       "829         NaN     Southampton"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"embark_town\"].isnull()][[\"embark_town\",\"new_embark_town\"]] #missing value replace with mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09a97d-1497-4137-8958-ba70cff886bd",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127bd4f9-6899-4c31-baf0-7ee1decf92bc",
   "metadata": {},
   "source": [
    "### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30289853-e00d-4547-9506-353ad82e0af4",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "Imbalanced data refers to a situation where the distribution of the target variable in a dataset is highly skewed, meaning that one class or label is over-represented relative to others.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to biased or unreliable model performance, with the model predictably favoring the majority class. For certain applications, such as fraud detection, predicting rare events like failure or cancer detection, an imbalanced dataset can be a significant issue since a minority class could be what you are trying to detect, and predicting it correctly is extremely important. In such cases, a model that performs well on the majority class but misses rare events can be especially dangerous\n",
    "\n",
    "Moreover, imbalanced data can also lead to the following issues:\n",
    "\n",
    "1.Overfitting: A model trained on imbalanced data may overfit to the majority class, as it has more data points to learn from.\n",
    "\n",
    "2.Poor generalization: A model trained on imbalanced data may not generalize well to new data, as it has not learned to identify the minority class accurately.\n",
    "\n",
    "3.Poor performance: A model trained on imbalanced data may have poor performance metrics such as recall, precision, and F1-score, which are commonly used for evaluating classification models.\n",
    "\n",
    "Therefore, it is essential to handle imbalanced data to improve the accuracy of the model's predictions and ensure that it performs well on both the majority and minority classes. There are several techniques for handling imbalanced data, such as undersampling, oversampling, and SMOTE. The choice of technique depends on the nature of the data and the analysis goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11311e39-e382-4478-bf30-ab2753223d7c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f33e50c-6d93-46ea-b039-633e10eca1ff",
   "metadata": {},
   "source": [
    "### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "### sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1e372-8794-4348-8467-d834b378fbfc",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "Up-sampling and down-sampling are techniques used in machine learning to handle imbalanced datasets. An imbalanced dataset is one where the number of examples in each class is not roughly equal. In this case, a machine learning algorithm might learn to predict the majority class, ignoring the minority class.\n",
    "\n",
    "1.Up-sampling:- involves increasing the number of examples in the minority class to balance the dataset. One way to do this is to create copies of the minority class examples. Another way is to use a generative model to generate new examples that look similar to the minority class examples.\n",
    "\n",
    "2.Down-sampling:- involves reducing the number of examples in the majority class to balance the dataset. One way to do this is to randomly remove examples from the majority class until the number of examples in each class is roughly equal.\n",
    "\n",
    "For example, suppose we have a dataset of 1000 images, of which 900 belong to class A and 100 belong to class B. This is an imbalanced dataset, because there are far more examples of class A than class B. To balance the dataset, we could down-sample the class A images to 100, or up-sample the class B images to 900.\n",
    "\n",
    "Both up-sampling and down-sampling have their advantages and disadvantages, and the choice of technique depends on the specifics of the dataset and the problem being solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b210c91-21c8-4a2c-bcb5-560a485bd4ab",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c528594-5e92-4d9e-ad5d-d4c2d01313e9",
   "metadata": {},
   "source": [
    "### Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2618c0-1833-47b9-9c06-054e92f5840b",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "->Data augmentation:- is a technique used in machine learning to artificially increase the size of a dataset by creating new examples from the existing data. This technique is often used when a dataset is small or imbalanced. It helps the machine learning algorithm to generalize better and to prevent overfitting on a small dataset.\n",
    "\n",
    "One common technique for data augmentation is known as SMOTE, which stands for Synthetic Minority Over-sampling Technique. SMOTE is specifically designed for balancing imbalanced datasets by generating synthetic examples for the minority class(es) while preserving the overall distribution of the dataset.\n",
    "\n",
    "->SMOTE works by creating synthetic examples for each minority class example. It does this by selecting a random minority class example and then selecting one of its k nearest neighbors. It then creates a new synthetic example by interpolating between the two examples using the formula:\n",
    "\n",
    "Overall, SMOTE is a powerful technique for balancing imbalanced datasets by generating synthetic data, and it can help improve the performance of machine learning algorithms on such datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be91a3cf-e119-41f9-9141-12d9c3199be7",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a34c3-d5fd-428a-9c44-3a67d93062f8",
   "metadata": {},
   "source": [
    "### Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17c8e5-23db-453b-abbc-f3263349c9d0",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "->Outliers in a dataset are data points that are significantly different from other data points in the dataset. Outliers can be caused by measurement errors, data corruption, or other anomalies in the data. It is important to handle outliers because they can have a large influence on statistical measures such as the mean, standard deviation, and correlation coefficients. Outliers can also affect the performance of machine learning algorithms, leading to poor predictions or models that overfit to the data.\n",
    "\n",
    "->Handling outliers involves identifying them in the dataset and deciding whether to remove them or modify them in some way. Depending on the nature of the data, outliers can be removed, replaced with missing values or imputed with plausible values. Alternatively, we can choose to leave them in the dataset, but use robust statistical methods that are less sensitive to outliers.\n",
    "\n",
    "In summary, handling outliers is an essential step in statistical analysis and machine learning since outliers can significantly impact the accuracy and reliability of a model or result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dae811-215a-44f9-89f9-3887f521ea7b",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18e30a-e2f9-4bd9-bce2-f868262e05b1",
   "metadata": {},
   "source": [
    "### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "### the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e238c8d-01e3-4256-b2d5-dffd51d1e196",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "Dealing with missing data is a common challenge in data analysis. Here are some techniques that can be used to handle missing data:\n",
    "\n",
    "1.Deleting the missing data: If the missing data is small in number, it can be removed from the dataset. This method is known as \"listwise deletion\" or \"complete-case analysis.\" However, this method can result in a loss of valuable information and reduce the sample size.\n",
    "\n",
    "2.Imputing the missing data: Imputation involves filling in the missing data with a reasonable estimate. Commonly used imputation techniques include mean imputation, mode imputation, median imputation, regression imputation, and k-nearest neighbor imputation.\n",
    "\n",
    "3.Using machine learning algorithms: Machine learning algorithms can be trained to predict the missing data using the available data. This approach can provide more accurate estimates and preserve the sample size.\n",
    "\n",
    "4.Analyzing the available data: If the missing data is limited to certain variables or features, it may be possible to analyze the available data without imputing the missing values. This method is known as \"available case analysis.\"\n",
    "\n",
    "5.Multiple imputation: Multiple imputation involves creating multiple plausible imputed datasets and combining the results to provide more robust estimates. This method can be computationally intensive but provides more accurate estimates.\n",
    "\n",
    "The choice of technique depends on the amount and pattern of missing data, the distribution of the variables, and the research question. It is important to carefully consider the pros and cons of each technique before selecting the most appropriate one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b037d-37b1-4f2e-ae01-4b95d3339f13",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76786f9-8b9a-447b-b535-6a55cd39b962",
   "metadata": {},
   "source": [
    "### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "### some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "### to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdef09-d76d-4074-a4b4-0c6f65e5e91a",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "There are several strategies that can be used to determine if the missing data is missing at random (MAR) or if there is a pattern to the missing data:\n",
    "\n",
    "1.Descriptive analysis: One way to detect a pattern in missing data is to perform a descriptive analysis of the dataset. This includes examining the distribution of missing data across variables and looking for any patterns or trends.\n",
    "\n",
    "2.Correlation analysis: Correlation analysis can be used to determine if there is a relationship between the missing data and other variables in the dataset. If there is a correlation between the missing data and other variables, it may suggest a non-random pattern to the missing data.\n",
    "\n",
    "3.Missingness tests: There are several statistical tests that can be used to determine if the missing data is missing at random. These tests include the Little's MCAR test, the Heckman two-stage method, and the Pattern-Mixture Model.\n",
    "\n",
    "4.Data imputation: Imputation methods can be used to fill in the missing data and assess the relationship between the imputed values and the other variables in the dataset. If the imputed values are significantly different from the actual values, it may suggest a non-random pattern to the missing data.\n",
    "\n",
    "5.Expert judgment: Domain experts can provide valuable insights into the nature of the missing data and help determine if it is missing at random or if there is a pattern to the missing data.\n",
    "\n",
    "It is important to carefully consider the nature and extent of the missing data and choose appropriate methods for detecting patterns in the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5441d5-b193-4e3b-b5f6-c3e990c365f7",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d1db40-dc20-4455-ada2-2963d30dbedf",
   "metadata": {},
   "source": [
    "### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "### dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "### can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c24b743-f12b-44e5-91fc-95ebe017f195",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "When working with an imbalanced dataset with a limited number of positive examples and a large number of negative examples, evaluation metrics such as accuracy can be misleading for assessing the performance of a machine learning model. Here are some strategies you can use to evaluate the performance of your model on such datasets:\n",
    "\n",
    "1.Confusion Matrix: Use a confusion matrix to assess the performance of your model. It will allow you to calculate metrics such as precision, recall, and F1-score, which are more informative than accuracy.\n",
    "\n",
    "2.Resampling Techniques: Use resampling techniques like oversampling or undersampling to balance the dataset. Oversampling involves replicating the minority class, while undersampling involves removing samples from the majority class. Careful consideration should be taken when using such techniques as the real-world data may be very different.\n",
    "\n",
    "3.Cost-sensitive Learning: Assign different weights to the positive and negative samples to help the model learn the minority class. This technique can help to reduce the impact of the class imbalance on the model's performance.\n",
    "\n",
    "4.Ensemble Methods: Use ensemble methods such as bagging or boosting to combine multiple weak models to create a strong model that performs well on the minority class.\n",
    "\n",
    "5.Anomaly Detection: Treat the minority class as anomalies and apply anomaly detection techniques.\n",
    "\n",
    "Overall, it is important to evaluate the model's performance metrics carefully when working with an imbalanced dataset. Choosing the right evaluation metric and employing techniques such as resampling, cost-sensitive learning or ensemble methods that have robustness to class imbalance can improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c73e639-c12e-41e3-9aae-21a4f3f05592",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12384e4c-ba98-42bc-9cca-b2865ea3d3b1",
   "metadata": {},
   "source": [
    "### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "### unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "### balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94777eae-3135-452f-99f8-2ee4c3f11529",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "To balance an unbalanced dataset, where one class is over-represented, one common method is to down-sample the majority class. Here are some techniques to achieve this:\n",
    "\n",
    "1.Random sampling: Randomly select a subset of the majority class, such that the number of samples in both classes is roughly equal.\n",
    "\n",
    "2.Cluster centroids: Cluster the majority class using K-means clustering and then down-sample each cluster's centroid.\n",
    "\n",
    "3.Tomek links: Identify pairs of nearest neighbors in the dataset and remove the one in the majority class, as these are likely to be outliers.\n",
    "\n",
    "4.NearMiss: Select samples from the majority class, such that they are closest to the minority class samples.\n",
    "\n",
    "5.Condensed Nearest Neighbor Rule (CNN): Recursively apply the 1-NN rule to select samples from the majority class that are incorrectly classified by the model.\n",
    "\n",
    "After downsampling the majority class, it is important to train and evaluate the model on the balanced dataset. Additionally, one can consider using other sampling techniques such as oversampling the minority class, generating synthetic samples using techniques like SMOTE, or using algorithms that are designed to handle imbalanced datasets such as XGBoost and LightGBM.\n",
    "\n",
    "Overall, it is important to carefully choose the appropriate sampling technique based on the characteristics of the dataset and the nature of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcb43e-352d-4d4b-a041-52ec1ed3946e",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f411486-87c3-4d10-a056-5c9eeb430363",
   "metadata": {},
   "source": [
    "### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "### project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "### balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8fcc4f-8638-4459-8f9d-08d9465d43a4",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "When dealing with an unbalanced dataset, where one class is significantly less frequent than the other, it can be challenging to estimate the occurrence of a rare event accurately. Here are some methods to balance the dataset and up-sample the minority class:\n",
    "\n",
    "1.Random over-sampling: This method involves randomly duplicating instances from the minority class to match the size of the majority class. This approach can increase the risk of overfitting and may not capture the full range of the minority class.\n",
    "\n",
    "2.Synthetic minority over-sampling technique (SMOTE): This method involves creating synthetic samples of the minority class by interpolating between existing samples. This approach can increase the size of the minority class without introducing bias.\n",
    "\n",
    "3.Adaptive synthetic sampling (ADASYN): This method is an extension of SMOTE that generates synthetic samples in regions of the feature space where the density of the minority class is low.\n",
    "\n",
    "4.Cost-sensitive learning: This approach involves assigning different costs to different types of errors in the model. This can help to balance the influence of the minority and majority classes in the model.\n",
    "\n",
    "5.Ensemble methods: Ensemble methods, such as bagging and boosting, can be used to balance the dataset by aggregating the results of multiple models trained on balanced subsets of the dataset.\n",
    "\n",
    "The choice of method depends on the nature of the dataset, the research question, and the performance of the model on the balanced dataset. It is important to carefully consider the trade-offs of each method and choose the most appropriate one for the specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537783ed-313f-47f4-9026-8fded978558e",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5c88b-340b-45a1-9043-1eb2692bf2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
